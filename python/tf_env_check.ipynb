{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Exercise\n",
    "\n",
    "https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/conda_lenjoy_tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import six.moves.urllib.request as request\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check that we have correct TensorFlow version installed\n",
    "tf_version = tf.__version__\n",
    "print(\"TensorFlow version: {}\".format(tf_version))\n",
    "assert \"1.4\" <= tf_version, \"TensorFlow r1.4 or later is needed\"\n",
    "\n",
    "# Windows users: You only need to change PATH, rest is platform independent\n",
    "PATH = \"/tmp/tf_dataset_and_estimator_apis\"\n",
    "\n",
    "# Fetch and store Training and Test dataset files\n",
    "PATH_DATASET = PATH + os.sep + \"dataset\"\n",
    "FILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\n",
    "FILE_TEST = PATH_DATASET + os.sep + \"iris_test.csv\"\n",
    "URL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "URL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "\n",
    "def download_dataset(url, file):\n",
    "    if not os.path.exists(PATH_DATASET):\n",
    "        os.makedirs(PATH_DATASET)\n",
    "    if not os.path.exists(file):\n",
    "        data = request.urlopen(url).read()\n",
    "        with open(file, \"wb\") as f:\n",
    "            f.write(data)\n",
    "            f.close()\n",
    "download_dataset(URL_TRAIN, FILE_TRAIN)\n",
    "download_dataset(URL_TEST, FILE_TEST)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "train_df = pandas.read_csv(FILE_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['120', '4', 'setosa', 'versicolor', 'virginica'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>120</th>\n",
       "      <th>4</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   120    4  setosa  versicolor  virginica\n",
       "0  6.4  2.8     5.6         2.2          2\n",
       "1  5.0  2.3     3.3         1.0          1\n",
       "2  4.9  2.5     4.5         1.7          2\n",
       "3  4.9  3.1     1.5         0.1          0\n",
       "4  5.7  3.8     1.7         0.3          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.columns)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_dataset_and_estimator_apis', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1272b9470>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_dataset_and_estimator_apis/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3001 into /tmp/tf_dataset_and_estimator_apis/model.ckpt.\n",
      "INFO:tensorflow:loss = 4.0749245, step = 3001\n",
      "INFO:tensorflow:global_step/sec: 508.352\n",
      "INFO:tensorflow:loss = 0.97058165, step = 3101 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 629.556\n",
      "INFO:tensorflow:loss = 0.73331296, step = 3201 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 634.155\n",
      "INFO:tensorflow:loss = 0.6599698, step = 3301 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 627.25\n",
      "INFO:tensorflow:loss = 0.90747136, step = 3401 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 606.262\n",
      "INFO:tensorflow:loss = 0.4658576, step = 3501 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 576.064\n",
      "INFO:tensorflow:loss = 4.492956, step = 3601 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 615.569\n",
      "INFO:tensorflow:loss = 4.9355736, step = 3701 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 648.25\n",
      "INFO:tensorflow:loss = 3.9130752, step = 3801 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.459\n",
      "INFO:tensorflow:loss = 4.281149, step = 3901 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 583.295\n",
      "INFO:tensorflow:loss = 0.7320999, step = 4001 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.558\n",
      "INFO:tensorflow:loss = 4.023569, step = 4101 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 635.731\n",
      "INFO:tensorflow:loss = 0.51117414, step = 4201 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.602\n",
      "INFO:tensorflow:loss = 0.42648906, step = 4301 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 579.226\n",
      "INFO:tensorflow:loss = 4.329241, step = 4401 (0.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 645.602\n",
      "INFO:tensorflow:loss = 1.0402596, step = 4501 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 545.167\n",
      "INFO:tensorflow:loss = 4.2999873, step = 4601 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 630.509\n",
      "INFO:tensorflow:loss = 3.8201144, step = 4701 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 493.798\n",
      "INFO:tensorflow:loss = 0.33584768, step = 4801 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 589.522\n",
      "INFO:tensorflow:loss = 4.3074193, step = 4901 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 453.985\n",
      "INFO:tensorflow:loss = 0.29476225, step = 5001 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 518.908\n",
      "INFO:tensorflow:loss = 4.1684055, step = 5101 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 557.74\n",
      "INFO:tensorflow:loss = 0.62445235, step = 5201 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 585.189\n",
      "INFO:tensorflow:loss = 4.1790795, step = 5301 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 606.285\n",
      "INFO:tensorflow:loss = 0.46510643, step = 5401 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 608.976\n",
      "INFO:tensorflow:loss = 0.75563926, step = 5501 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 630.879\n",
      "INFO:tensorflow:loss = 4.5020285, step = 5601 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 613.057\n",
      "INFO:tensorflow:loss = 4.1576605, step = 5701 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 586.353\n",
      "INFO:tensorflow:loss = 0.8214567, step = 5801 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 571.474\n",
      "INFO:tensorflow:loss = 0.5774933, step = 5901 (0.175 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6000 into /tmp/tf_dataset_and_estimator_apis/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.0037384.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-24-01:16:01\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_dataset_and_estimator_apis/model.ckpt-6000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-24-01:16:01\n",
      "INFO:tensorflow:Saving dict for global step 6000: accuracy = 0.96666664, average_loss = 0.059885107, global_step = 6000, loss = 1.7965533\n",
      "Evaluation results\n",
      "   accuracy, was: 0.9666666388511658\n",
      "   average_loss, was: 0.05988510698080063\n",
      "   loss, was: 1.7965532541275024\n",
      "   global_step, was: 6000\n",
      "Predictions on test file\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_dataset_and_estimator_apis/model.ckpt-6000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# The CSV features in our training & test data\n",
    "feature_names = [\n",
    "    'SepalLength',\n",
    "    'SepalWidth',\n",
    "    'PetalLength',\n",
    "    'PetalWidth']\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "\n",
    "\n",
    "def my_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    def decode_csv(line):\n",
    "        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
    "        label = parsed_line[-1]  # Last element is the label\n",
    "        del parsed_line[-1]  # Delete last element\n",
    "        features = parsed_line  # Everything but last elements are the features\n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d\n",
    "\n",
    "    dataset = (tf.data.TextLineDataset(file_path)  # Read text file\n",
    "               .skip(1)  # Skip header row\n",
    "               .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
    "    dataset = dataset.batch(32)  # Batch size to use\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "\n",
    "next_batch = my_input_fn(FILE_TRAIN, True)  # Will return 32 random elements\n",
    "\n",
    "# Create the feature_columns, which specifies the input to our model\n",
    "# All our input features are numeric, so use numeric_column for each one\n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]\n",
    "\n",
    "# Create a deep neural network regression classifier\n",
    "# Use the DNNClassifier pre-made estimator\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,  # The input features to our model\n",
    "    hidden_units=[10, 10],  # N layers, each with 10 neurons\n",
    "    n_classes=3,\n",
    "    model_dir=PATH)  # Path to where checkpoints etc are stored\n",
    "\n",
    "# Train our model, use the previously defined function my_input_fn\n",
    "# Input to training is a file with training example\n",
    "# Stop training after 8 iterations of train data (epochs)\n",
    "classifier.train(\n",
    "    input_fn=lambda: my_input_fn(FILE_TRAIN, True, 800))\n",
    "\n",
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# Return value will contain evaluation_metrics such as: loss & average_loss\n",
    "evaluate_result = classifier.evaluate(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, False, 4))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))\n",
    "\n",
    "# Predict the type of some Iris flowers.\n",
    "# Let's predict the examples in FILE_TEST, repeat only once.\n",
    "predict_results = classifier.predict(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, False, 1))\n",
    "print(\"Predictions on test file\")\n",
    "for prediction in predict_results:\n",
    "    # Will print the predicted class, i.e: 0, 1, or 2 if the prediction\n",
    "    # is Iris Sentosa, Vericolor, Virginica, respectively.\n",
    "    print(prediction[\"class_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_dataset_and_estimator_apis/model.ckpt-6000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "  I think: [5.9, 3.0, 4.2, 1.5], is Iris Versicolor\n",
      "  I think: [6.9, 3.1, 5.4, 2.1], is Iris Virginica\n",
      "  I think: [5.1, 3.3, 1.7, 0.5], is Iris Sentosa\n"
     ]
    }
   ],
   "source": [
    "# Let create a dataset for prediction\n",
    "# We've taken the first 3 examples in FILE_TEST\n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa\n",
    "\n",
    "\n",
    "def new_input_fn():\n",
    "    def decode(x):\n",
    "        x = tf.split(x, 4)  # Need to split into our 4 features\n",
    "        return dict(zip(feature_names, x))  # To build a dict of them\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature_batch = iterator.get_next()\n",
    "    return next_feature_batch, None  # In prediction, we have no labels\n",
    "\n",
    "\n",
    "# Predict all our prediction_input\n",
    "predict_results = classifier.predict(input_fn=new_input_fn)\n",
    "\n",
    "# Print results\n",
    "print(\"Predictions:\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    type = prediction[\"class_ids\"][0]  # Get the predicted class (index)\n",
    "    if type == 0:\n",
    "        print(\"  I think: {}, is Iris Sentosa\".format(prediction_input[idx]))\n",
    "    elif type == 1:\n",
    "        print(\"  I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
    "    else:\n",
    "        print(\"  I think: {}, is Iris Virginica\".format(prediction_input[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
