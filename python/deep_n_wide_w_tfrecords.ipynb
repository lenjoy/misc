{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep & Wide Model with TFRecords\n",
    "\n",
    "Derived from https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "\n",
    "Update the example by using `TFRecords` format in Dataset API\n",
    "\n",
    "Demo a multi-hot column (a string of tag array with a delimiter) for SparseTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf  # pylint: disable=g-bad-import-order\n",
    "\n",
    "\n",
    "_CSV_COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "\n",
    "_CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                        [0], [0], [0], [''], ['']]\n",
    "\n",
    "_NUM_EXAMPLES = {\n",
    "    'train': 32561,\n",
    "    'validation': 16281,\n",
    "}\n",
    "\n",
    "\n",
    "LOSS_PREFIX = {'wide': 'linear/', 'deep': 'dnn/'}\n",
    "\n",
    "\n",
    "def build_model_columns():\n",
    "  \"\"\"Builds a set of wide and deep feature columns.\"\"\"\n",
    "  # Continuous columns\n",
    "  age = tf.feature_column.numeric_column('age')\n",
    "  education_num = tf.feature_column.numeric_column('education_num')\n",
    "  capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
    "  capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
    "  hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
    "\n",
    "  education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'education', [\n",
    "          'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
    "          'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
    "          '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
    "\n",
    "  marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'marital_status', [\n",
    "          'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
    "          'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
    "\n",
    "  relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'relationship', [\n",
    "          'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
    "          'Other-relative'])\n",
    "\n",
    "  workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'workclass', [\n",
    "          'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
    "          'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n",
    "\n",
    "  # To show an example of hashing:\n",
    "  occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "      'occupation', hash_bucket_size=1000)\n",
    "\n",
    "  # multi-hot\n",
    "  occupation_sp = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "      'occupation_sp', hash_bucket_size=1000)\n",
    "\n",
    "  # Transformations.\n",
    "  age_buckets = tf.feature_column.bucketized_column(\n",
    "      age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "\n",
    "  # Wide columns and deep columns.\n",
    "  base_columns = [\n",
    "      education, marital_status, relationship, workclass, occupation,\n",
    "      age_buckets,\n",
    "  ]\n",
    "\n",
    "  crossed_columns = [\n",
    "      tf.feature_column.crossed_column(\n",
    "          ['education', 'occupation'], hash_bucket_size=1000),\n",
    "      tf.feature_column.crossed_column(\n",
    "          [age_buckets, 'education', 'occupation'], hash_bucket_size=1000),\n",
    "  ]\n",
    "\n",
    "  wide_columns = base_columns + crossed_columns\n",
    "\n",
    "  deep_columns = [\n",
    "      age,\n",
    "      education_num,\n",
    "      capital_gain,\n",
    "      capital_loss,\n",
    "      hours_per_week,\n",
    "      tf.feature_column.indicator_column(workclass),\n",
    "      tf.feature_column.indicator_column(education),\n",
    "      tf.feature_column.indicator_column(marital_status),\n",
    "      tf.feature_column.indicator_column(relationship),\n",
    "      tf.feature_column.indicator_column(occupation_sp),\n",
    "\n",
    "      # To show an example of embedding\n",
    "      tf.feature_column.embedding_column(occupation, dimension=8),\n",
    "  ]\n",
    "\n",
    "  return wide_columns, deep_columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, model_type):\n",
    "  \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n",
    "  wide_columns, deep_columns = build_model_columns()\n",
    "  hidden_units = [100, 75, 50, 25]\n",
    "\n",
    "  # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n",
    "  # trains faster than GPU for this model.\n",
    "  run_config = tf.estimator.RunConfig().replace(\n",
    "      session_config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "\n",
    "  if model_type == 'wide':\n",
    "    return tf.estimator.LinearClassifier(\n",
    "        model_dir=model_dir,\n",
    "        feature_columns=wide_columns,\n",
    "        config=run_config)\n",
    "  elif model_type == 'deep':\n",
    "    return tf.estimator.DNNClassifier(\n",
    "        model_dir=model_dir,\n",
    "        feature_columns=deep_columns,\n",
    "        hidden_units=hidden_units,\n",
    "        config=run_config)\n",
    "  else:\n",
    "    return tf.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=hidden_units,\n",
    "        config=run_config)\n",
    "\n",
    "\n",
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "    \"\"\"Generate an input function for the Estimator.\"\"\"\n",
    "\n",
    "    assert tf.gfile.Exists(data_file), (\n",
    "        '%s not found. Please make sure you have run data_download.py and '\n",
    "        'set the --data_dir argument to the correct path.' % data_file)\n",
    "\n",
    "    table = tf.contrib.lookup.index_table_from_file(\n",
    "        vocabulary_file='test.txt', num_oov_buckets=1)\n",
    "\n",
    "    def trans_tensor1(table, split_tags):\n",
    "        tags_value = table.lookup(split_tags.values)\n",
    "        categorial_tensor = tf.SparseTensor(\n",
    "            indices=split_tags.indices,\n",
    "            values=tags_value,\n",
    "            dense_shape=split_tags.dense_shape)\n",
    "        return categorial_tensor\n",
    "\n",
    "    def trans_tensor2(table, split_tags):\n",
    "        tags_value = table.lookup(split_tags.values)\n",
    "\n",
    "        # Output: tags.indices Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64)\n",
    "        print('tags.indices', split_tags.indices)\n",
    "        print('tags_value', tags_value)\n",
    "        \n",
    "        indice_idx = tf.map_fn(lambda x : x[0], tags.indices)\n",
    "        print('indice_idx', indice_idx)\n",
    "        value_idx = tf.map_fn(lambda x : x[1], tags.indices)\n",
    "        print('value_idx', value_idx)\n",
    "        \n",
    "        value_arr = tf.cast(tf.gather(tags.values, value_idx), tf.int64)\n",
    "        print('value_arr shape', value_arr.shape)\n",
    "\n",
    "        new_indices = tf.stack([indice_idx, value_arr], axis=1)\n",
    "        print('new_indices', new_indices)\n",
    "        # new_values = [1 for x in range(value_arr.shape[0])]\n",
    "        new_values = tf.ones_like(tags_value, tf.int64)\n",
    "        print('new_values', new_values)\n",
    "\n",
    "        print('split_tags shape', split_tags.get_shape())\n",
    "        print('value_arr shape', value_arr.get_shape())\n",
    "        print('new_indices shape', new_indices.get_shape())\n",
    "\n",
    "        categorial_tensor = tf.SparseTensor(indices=new_indices,\n",
    "                                            values=new_values,\n",
    "                                            dense_shape=[new_indices.shape[1], 4])\n",
    "        return categorial_tensor\n",
    "\n",
    "    def parse_csv(value):\n",
    "        print('Parsing', data_file)\n",
    "        columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "        features = dict(zip(_CSV_COLUMNS, columns))\n",
    "\n",
    "        # support multi-hot sparse features\n",
    "        split_tags = tf.string_split([columns[6]], \"-\")\n",
    "        print('tags.indices', tags.indices)\n",
    "\n",
    "        categorial_tensor = trans_tensor1(table, split_tags)\n",
    "\n",
    "#        with tf.Session() as s:\n",
    "#            s.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#            print(s.run(split_tags))\n",
    "        \n",
    "        categorical_cols = {\n",
    "            'occupation_sp': categorial_tensor}\n",
    "        features.update(categorical_cols)\n",
    "        \n",
    "        labels = features.pop('income_bracket')\n",
    "        return features, tf.equal(labels, '>50K')\n",
    "\n",
    "    # Extract lines from input files using the Dataset API.\n",
    "    dataset = tf.data.TextLineDataset(data_file)\n",
    "\n",
    "    # if shuffle:\n",
    "    #     dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n",
    "\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_input_fn(data_file, num_epochs, batch_size):\n",
    "    dataset = tf.data.TFRecordDataset(data_file)\n",
    "\n",
    "    # Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "    # protocol buffer, and perform any additional per-record preprocessing.\n",
    "    def parse_tfrecords(record):\n",
    "        keys_to_features = {\n",
    "            'age': tf.FixedLenFeature(\n",
    "                (), tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "            'education_num': tf.FixedLenFeature(\n",
    "                (), tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "            'capital_gain': tf.FixedLenFeature(\n",
    "                (), tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "            'capital_loss': tf.FixedLenFeature(\n",
    "                (), tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "            'hours_per_week': tf.FixedLenFeature(\n",
    "                (), tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "            'education': tf.FixedLenFeature(\n",
    "                (), tf.string),\n",
    "            'marital_status': tf.FixedLenFeature(\n",
    "                (), tf.string),\n",
    "            'relationship': tf.FixedLenFeature(\n",
    "                (), tf.string),\n",
    "            'workclass': tf.FixedLenFeature(\n",
    "                (), tf.string),\n",
    "            'occupation': tf.FixedLenFeature((), tf.string),\n",
    "            'occupation_sp': tf.VarLenFeature(tf.string),\n",
    "            'income_bracket': tf.FixedLenFeature(\n",
    "                (), tf.string),\n",
    "        }\n",
    "        parsed = tf.parse_single_example(record, keys_to_features)\n",
    "        return {\n",
    "            'age': parsed['age'],\n",
    "            'education_num': parsed['education_num'],\n",
    "            'capital_gain': parsed['capital_gain'],\n",
    "            'capital_loss': parsed['capital_loss'],\n",
    "            'hours_per_week': parsed['hours_per_week'],\n",
    "            'education': parsed['education'],\n",
    "            'marital_status': parsed['marital_status'],\n",
    "            'relationship': parsed['relationship'],\n",
    "            'workclass': parsed['workclass'],\n",
    "            'occupation': parsed['occupation'],\n",
    "            'occupation_sp': parsed['occupation_sp'],\n",
    "               }, tf.equal(parsed['income_bracket'], '>50K')\n",
    "\n",
    "    # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "    # tensor for each example.\n",
    "    dataset = dataset.map(parse_tfrecords)\n",
    "    dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    return dataset\n",
    "    \n",
    "    # iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    # `features` is a dictionary in which each value is a batch of values for\n",
    "    # that feature; `labels` is a batch of labels.\n",
    "    # features, labels = iterator.get_next()\n",
    "    # return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/census_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {\n",
      "  key: \"GPU\"\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x10aa5b1d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/census_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 269.8815, step = 1\n",
      "INFO:tensorflow:global_step/sec: 46.0251\n",
      "INFO:tensorflow:loss = 27.836712, step = 101 (2.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.92\n",
      "INFO:tensorflow:loss = 17.604763, step = 201 (0.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.054\n",
      "INFO:tensorflow:loss = 19.226704, step = 301 (0.833 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.99\n",
      "INFO:tensorflow:loss = 17.235504, step = 401 (0.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.0165\n",
      "INFO:tensorflow:loss = 15.117376, step = 501 (1.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9013\n",
      "INFO:tensorflow:loss = 14.11949, step = 601 (1.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.06\n",
      "INFO:tensorflow:loss = 10.596415, step = 701 (0.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.633\n",
      "INFO:tensorflow:loss = 16.486126, step = 801 (0.759 sec)\n",
      "INFO:tensorflow:global_step/sec: 123.452\n",
      "INFO:tensorflow:loss = 29.196747, step = 901 (0.810 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.082\n",
      "INFO:tensorflow:loss = 17.425045, step = 1001 (0.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.946\n",
      "INFO:tensorflow:loss = 12.44164, step = 1101 (0.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.4542\n",
      "INFO:tensorflow:loss = 13.629208, step = 1201 (1.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.379\n",
      "INFO:tensorflow:loss = 12.775593, step = 1301 (1.063 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.276\n",
      "INFO:tensorflow:loss = 14.457914, step = 1401 (0.936 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.58\n",
      "INFO:tensorflow:loss = 9.28247, step = 1501 (0.858 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.936\n",
      "INFO:tensorflow:loss = 12.67549, step = 1601 (0.848 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1630 into /tmp/census_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.05236806.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-05:44:50\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/census_model/model.ckpt-1630\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-05:45:07\n",
      "INFO:tensorflow:Saving dict for global step 1630: accuracy = 0.8379092, accuracy_baseline = 0.76377374, auc = 0.88590914, auc_precision_recall = 0.72878367, average_loss = 0.35582113, global_step = 1630, label/mean = 0.23622628, loss = 14.198833, prediction/mean = 0.26431632\n",
      "Results at epoch 2\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.8379092\n",
      "accuracy_baseline: 0.76377374\n",
      "auc: 0.88590914\n",
      "auc_precision_recall: 0.72878367\n",
      "average_loss: 0.35582113\n",
      "global_step: 1630\n",
      "label/mean: 0.23622628\n",
      "loss: 14.198833\n",
      "prediction/mean: 0.26431632\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/census_model/model.ckpt-1630\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1631 into /tmp/census_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 14.939872, step = 1631\n",
      "INFO:tensorflow:global_step/sec: 17.7128\n",
      "INFO:tensorflow:loss = 11.752113, step = 1731 (5.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.7464\n",
      "INFO:tensorflow:loss = 12.476782, step = 1831 (1.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.8185\n",
      "INFO:tensorflow:loss = 10.544, step = 1931 (2.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.188\n",
      "INFO:tensorflow:loss = 10.593834, step = 2031 (1.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.1727\n",
      "INFO:tensorflow:loss = 22.763567, step = 2131 (1.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.8396\n",
      "INFO:tensorflow:loss = 14.178909, step = 2231 (1.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.3483\n",
      "INFO:tensorflow:loss = 8.046773, step = 2331 (1.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.1064\n",
      "INFO:tensorflow:loss = 10.618149, step = 2431 (1.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.1088\n",
      "INFO:tensorflow:loss = 7.968749, step = 2531 (1.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.8045\n",
      "INFO:tensorflow:loss = 8.62059, step = 2631 (1.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.4414\n",
      "INFO:tensorflow:loss = 12.632227, step = 2731 (1.654 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0133\n",
      "INFO:tensorflow:loss = 16.152386, step = 2831 (1.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.1409\n",
      "INFO:tensorflow:loss = 14.7097645, step = 2931 (1.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.6322\n",
      "INFO:tensorflow:loss = 12.810727, step = 3031 (1.571 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.2196\n",
      "INFO:tensorflow:loss = 9.840369, step = 3131 (1.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.5296\n",
      "INFO:tensorflow:loss = 18.220442, step = 3231 (1.183 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3260 into /tmp/census_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.052870784.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-05:45:53\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/census_model/model.ckpt-3260\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-05:46:04\n",
      "INFO:tensorflow:Saving dict for global step 3260: accuracy = 0.84988636, accuracy_baseline = 0.76377374, auc = 0.89794683, auc_precision_recall = 0.7606181, average_loss = 0.3332321, global_step = 3260, label/mean = 0.23622628, loss = 13.297432, prediction/mean = 0.24752685\n",
      "Results at epoch 4\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.84988636\n",
      "accuracy_baseline: 0.76377374\n",
      "auc: 0.89794683\n",
      "auc_precision_recall: 0.7606181\n",
      "average_loss: 0.3332321\n",
      "global_step: 3260\n",
      "label/mean: 0.23622628\n",
      "loss: 13.297432\n",
      "prediction/mean: 0.24752685\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model_dir = '/tmp/census_model'\n",
    "    data_dir = '/tmp/census_data'\n",
    "    model_type = 'wide_deep'\n",
    "    train_epochs = 4\n",
    "    epochs_between_evals = 2\n",
    "    batch_size = 40\n",
    "\n",
    "    # Clean up the model directory if present\n",
    "    shutil.rmtree(model_dir, ignore_errors=True)\n",
    "    model = build_estimator(model_dir, model_type)\n",
    "\n",
    "    # train_file = os.path.join(data_dir, 'adult.data')\n",
    "    # test_file = os.path.join(data_dir, 'adult.test')\n",
    "    train_file = os.path.join(data_dir, 'adult.data.tfrecords')\n",
    "    test_file = os.path.join(data_dir, 'adult.test.tfrecords')\n",
    "\n",
    "    # Train and evaluate the model every `flags.epochs_between_evals` epochs.\n",
    "    def train_input_fn():\n",
    "        # return input_fn(train_file, epochs_between_evals, True, batch_size)\n",
    "        return dataset_input_fn(train_file, epochs_between_evals, batch_size)\n",
    "\n",
    "    def eval_input_fn():\n",
    "        # return input_fn(test_file, 1, False, batch_size)\n",
    "        return dataset_input_fn(test_file, epochs_between_evals, batch_size)\n",
    "    \n",
    "    # Train and evaluate the model every `flags.epochs_between_evals` epochs.\n",
    "    for n in range(train_epochs // epochs_between_evals):\n",
    "        model.train(input_fn=train_input_fn)\n",
    "        results = model.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "        # Display evaluation metrics\n",
    "        print('Results at epoch', (n + 1) * epochs_between_evals)\n",
    "        print('-' * 60)\n",
    "\n",
    "        for key in sorted(results):\n",
    "            print('%s: %s' % (key, results[key]))\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
